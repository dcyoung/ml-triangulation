{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO List:\n",
    "- Try normalizing all samples to use coordinates relative to an origin at the centroid of the 3 participating hotspots for that sample\n",
    "- Try clustering or creating binned \"zones\" for lat/long that are directly predicted like tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from common import geo_to_cartesian_m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_EARTH_KM = 6373.0\n",
    "RADIUS_EARTH_M = RADIUS_EARTH_KM * 1000\n",
    "\n",
    "def enforce_safe_lat_long(lat_long_deg):\n",
    "    \"\"\"\n",
    "    Normalize point to [-90, 90] latitude and [-180, 180] longitude.\n",
    "    \"\"\"\n",
    "    lat_long_deg = 1.0 * lat_long_deg\n",
    "    lat_long_deg[:, 0] = (lat_long_deg[:, 0] + 90) % 360 - 90\n",
    "    mask = lat_long_deg[:, 0] > 90\n",
    "    lat_long_deg[mask, 0] = 180 - lat_long_deg[mask, 0]\n",
    "    lat_long_deg[mask, 1] += 180\n",
    "    lat_long_deg[:, 1] = (lat_long_deg[:, 1] + 180) % 360 - 180\n",
    "    return lat_long_deg\n",
    "\n",
    "# Assuming normalized lat/long contains values in range -1:1... return de-normalized values as true lat/long\n",
    "def denorm_lat_long(normalized_lat_long):\n",
    "    lat_long = (normalized_lat_long + 1.0) / 2.0\n",
    "    lat_long[:, 0] *= 180.0\n",
    "    lat_long[:, 0] -= 90.0\n",
    "    lat_long[:, 1] *= 360.0\n",
    "    lat_long[:, 1] -= 180.0\n",
    "    return enforce_safe_lat_long(lat_long)\n",
    "\n",
    "def normalize_lat_long(lat_long_deg):\n",
    "    \"\"\" Normalizes lat/long to range -1:1 \"\"\"\n",
    "    lat_long = enforce_safe_lat_long(lat_long_deg=lat_long_deg)\n",
    "    lat_long[:,0] = 1 - 2 * (lat_long[:,0] + 90.0) / 180.0\n",
    "    lat_long[:,1] = 1 - 2 * (lat_long[:,1] + 180.0) / 360.0\n",
    "    return lat_long\n",
    "\n",
    "def convert_lat_long_to_cart_xyz_TORCH(lat_long_deg: Tensor)->Tensor:\n",
    "    lat = torch.deg2rad(lat_long_deg[:,0])\n",
    "    lon = torch.deg2rad(lat_long_deg[:,1])\n",
    "    return torch.stack([\n",
    "        torch.sin(np.pi / 2.0 - lat) * torch.cos(lon), # x\n",
    "        torch.sin(np.pi / 2.0 - lat) * torch.sin(lon), # y\n",
    "        torch.cos(np.pi / 2.0 - lat)                   # z\n",
    "    ]).T\n",
    "\n",
    "\n",
    "def haversine_np(lat_long_deg_1, lat_long_deg_2, radius:float=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on a sphere\n",
    "    ie: Shortest distance between two points on the surface of a sphere\n",
    "    \"\"\"\n",
    "    lat_1, lon_1, lat_2, lon_2 = map(np.deg2rad, [lat_long_deg_1[:,0], lat_long_deg_1[:,1], lat_long_deg_2[:,0], lat_long_deg_2[:,1]])\n",
    "    d = np.sin((lat_2 - lat_1)/2)**2 + np.cos(lat_1)*np.cos(lat_2) * np.sin((lon_2 - lon_1)/2)**2\n",
    "    arc_len = 2 * radius * np.arcsin(np.sqrt(d))\n",
    "    return arc_len\n",
    "    \n",
    "def haversine_torch(lat_long_deg_1:Tensor, lat_long_deg_2:Tensor, radius:float=1.0)->Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on a sphere\n",
    "    ie: Shortest distance between two points on the surface of a sphere\n",
    "    \"\"\"\n",
    "    lat_1, lon_1, lat_2, lon_2 = map(torch.deg2rad, [lat_long_deg_1[:,0], lat_long_deg_1[:,1], lat_long_deg_2[:,0], lat_long_deg_2[:,1]])\n",
    "    d = torch.sin((lat_2 - lat_1)/2)**2 + torch.cos(lat_1)*torch.cos(lat_2) * torch.sin((lon_2 - lon_1)/2)**2\n",
    "    arc_len = 2 * radius * torch.arcsin(torch.sqrt(d))\n",
    "    return arc_len\n",
    "\n",
    "\n",
    "def equirectangular_np(lat_long_deg_1, lat_long_deg_2, radius:float=1.0):\n",
    "    \"\"\"\n",
    "    Simplified version of haversine for small distances where \n",
    "    Pythagoras theorem can be used on an equirectangular projection\n",
    "    \"\"\"\n",
    "    lat_1, lon_1, lat_2, lon_2 = map(np.deg2rad, [lat_long_deg_1[:,0], lat_long_deg_1[:,1], lat_long_deg_2[:,0], lat_long_deg_2[:,1]])\n",
    "    x = (lon_2 - lon_1) * np.cos(0.5 * (lat_2 + lat_1))\n",
    "    y = lat_2 - lat_1\n",
    "    return radius * np.sqrt(x*x + y*y)\n",
    "    \n",
    "def equirectangular_torch(lat_long_deg_1:Tensor, lat_long_deg_2:Tensor, radius:float=1.0)->Tensor:\n",
    "    \"\"\"\n",
    "    Simplified version of haversine for small distances where \n",
    "    Pythagoras theorem can be used on an equirectangular projection\n",
    "    \"\"\"\n",
    "    lat_1, lon_1, lat_2, lon_2 = map(torch.deg2rad, [lat_long_deg_1[:,0], lat_long_deg_1[:,1], lat_long_deg_2[:,0], lat_long_deg_2[:,1]])\n",
    "    x = (lon_2 - lon_1) * torch.cos(0.5 * (lat_2 + lat_1))\n",
    "    y = lat_2 - lat_1\n",
    "    return radius * torch.sqrt(x*x + y*y)\n",
    "\n",
    "def str_contains_sub(s, subs)->bool:\n",
    "    if isinstance(subs, str):\n",
    "        subs = [subs]\n",
    "    for sub in subs:\n",
    "        if sub in s:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/mappers_processed.csv\")\n",
    "if \"id\" in data.columns:\n",
    "    data = data.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "subset = data.sample(n=min(len(data), 2500))\n",
    "for i in range(1,4):\n",
    "    plt.scatter(subset[f\"hotspot{i}_lat\"], subset[f\"hotspot{i}_long\"], s=2, alpha=0.8, label=f\"hotspot{i}\")\n",
    "plt.scatter(subset[\"target_lat\"], subset[\"target_long\"], s=2, label=\"label\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine_vector, Unit\n",
    "# For each sample, calculate an average haversine distance between the target and the hotspot centroid\n",
    "ll_target = data[[\"target_lat\", \"target_long\"]].to_numpy()\n",
    "ll_centroid = data[[\"centroid_projected_lat\", \"centroid_projected_long\"]].to_numpy()\n",
    "dist_to_centroid_km = haversine_vector(\n",
    "    array1=ll_target, array2=ll_centroid, unit=Unit.KILOMETERS\n",
    ")\n",
    "data[\"dist_to_centroid_km\"] = dist_to_centroid_km\n",
    "for q in [0.5, 0.9, 0.99, 0.995, 0.999, 0.9999]:\n",
    "    km = np.quantile(data[\"dist_to_centroid_km\"], q)\n",
    "    print(f\"@ q={q},\\t km={km:.2f}km\")\n",
    "\n",
    "plt.hist(dist_to_centroid_km, bins=100)\n",
    "plt.ylabel(\"# samples\")\n",
    "plt.xlabel(\"Dist (km) from target to hotspot centroid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data by REMOVING outliers that represent EXTREMELY distant targets\n",
    "q = 0.995\n",
    "threshold_km = np.quantile(data[\"dist_to_centroid_km\"], q)\n",
    "n_before = len(data)\n",
    "data = data[data[\"dist_to_centroid_km\"] <= threshold_km]\n",
    "after = data.pop(\"dist_to_centroid_km\")\n",
    "print(f\"Removed {n_before - len(after)} sample.\")\n",
    "plt.hist(after, bins=100)\n",
    "plt.ylabel(\"# samples\")\n",
    "plt.xlabel(\"Dist (km) from target to hotspot centroid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical spreading factors to one-hot\n",
    "sf_original = data.pop(\"spreading_factor\")\n",
    "data = pd.concat([data, sf_original.str.get_dummies()], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"naive_tri_alt\" in data.columns:\n",
    "    data = data.rename(columns={\"naive_tri_alt\": \"naive_tri_altitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cartesian coords\n",
    "for prefix in [\"target\", \"hotspot1\", \"hotspot2\", \"hotspot3\", \"centroid_projected\", \"naive_tri\"]:\n",
    "    xyz_m = geo_to_cartesian_m(lat_long_alt=data[[f\"{prefix}_lat\", f\"{prefix}_long\", f\"{prefix}_altitude\"]].to_numpy())\n",
    "    xyz_norm = xyz_m / np.linalg.norm(xyz_m, axis=1)[:, np.newaxis]\n",
    "    data[f\"{prefix}_x_m\"] = xyz_m[:,0]\n",
    "    data[f\"{prefix}_y_m\"] = xyz_m[:,1]\n",
    "    data[f\"{prefix}_z_m\"] = xyz_m[:,2]\n",
    "    data[f\"{prefix}_x_norm\"] = xyz_norm[:,0]\n",
    "    data[f\"{prefix}_y_norm\"] = xyz_norm[:,1]\n",
    "    data[f\"{prefix}_z_norm\"] = xyz_norm[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalized lat, long\n",
    "for prefix in [\"target\", \"hotspot1\", \"hotspot2\", \"hotspot3\", \"centroid_projected\", \"naive_tri\"]:\n",
    "    # Normalize lat values\n",
    "    data[f\"{prefix}_lat_norm\"] = data[f\"{prefix}_lat\"] = 1 - 2 * (data[f\"{prefix}_lat\"] + 90.0) / 180.0\n",
    "    data[f\"{prefix}_long_norm\"] = data[f\"{prefix}_long\"] = 1 - 2 * (data[f\"{prefix}_long\"] + 180.0) / 360.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test with 80 train / 20 test\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the labels (latitude and longitude)\n",
    "# DATA_MODE = \"geo\"\n",
    "DATA_MODE = \"cartesian\"\n",
    "if DATA_MODE == \"geo\":\n",
    "        label_cols = [\"target_lat_norm\", \"target_long_norm\"]\n",
    "        sample_cols = [\n",
    "                # 'frequency',\n",
    "                # 'SF10BW125', 'SF11BW125', 'SF12BW125', 'SF7BW125', 'SF8BW125', 'SF8BW500', 'SF9BW125',\n",
    "                'centroid_projected_lat_norm', 'centroid_projected_long_norm',\n",
    "                'hotspot1_lat_norm', 'hotspot1_long_norm', \n",
    "                'hotspot2_lat_norm', 'hotspot2_long_norm', \n",
    "                'hotspot3_lat_norm', 'hotspot3_long_norm', \n",
    "                'hotspot1_rssi', 'hotspot2_rssi', 'hotspot3_rssi', \n",
    "                'hotspot1_snr', 'hotspot2_snr','hotspot3_snr',\n",
    "                'hotspot1_fspl_dist_km', 'hotspot2_fspl_dist_km', 'hotspot3_fspl_dist_km',\n",
    "                'naive_tri_lat_norm', 'naive_tri_long_norm',\n",
    "        ]\n",
    "        naive_tri_cols = [\"naive_tri_lat_norm\", \"naive_tri_long_norm\"]\n",
    "elif DATA_MODE == \"cartesian\":\n",
    "        label_cols = [\"target_x_norm\", \"target_y_norm\", \"target_z_norm\"]\n",
    "        sample_cols = [\n",
    "                \"centroid_projected_x_norm\", \"centroid_projected_y_norm\", \n",
    "                \"naive_tri_x_norm\", \"naive_tri_y_norm\", \"naive_tri_z_norm\", \n",
    "                'hotspot1_x_norm', 'hotspot1_y_norm','hotspot1_z_norm', \n",
    "                'hotspot2_x_norm', 'hotspot2_y_norm','hotspot2_z_norm', \n",
    "                'hotspot3_x_norm', 'hotspot3_y_norm','hotspot3_z_norm', \n",
    "                'hotspot1_rssi', 'hotspot2_rssi', 'hotspot3_rssi', \n",
    "                'hotspot1_snr', 'hotspot2_snr','hotspot3_snr',\n",
    "                'hotspot1_fspl_dist_km', 'hotspot2_fspl_dist_km', 'hotspot3_fspl_dist_km',\n",
    "        ]\n",
    "        naive_tri_cols = [\"naive_tri_x_norm\", \"naive_tri_y_norm\", \"naive_tri_z_norm\"]\n",
    "else:\n",
    "        raise ValueError(f\"Unexpected mode: {DATA_MODE}\")\n",
    "y_train = data_train[label_cols].to_numpy()\n",
    "y_val = data_val[label_cols].to_numpy()\n",
    "y_val_naive = data_val[naive_tri_cols].to_numpy()\n",
    "\n",
    "print(y_train.shape, y_train.min(), y_train.max())\n",
    "print(y_val.shape, y_val.min(), y_val.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the samples (data features, without labels)\n",
    "x_train = np.array(data_train[sample_cols])\n",
    "x_val = np.array(data_val[sample_cols])\n",
    "\n",
    "# Normalize all features to expected range -1:1 based on consideration of the training data alone\n",
    "X_TRAIN_MIN = x_train.min(axis=0)\n",
    "X_TRAIN_RANGE = x_train.ptp(axis=0)\n",
    "\n",
    "for i, s in enumerate(sample_cols):\n",
    "    # Avoid re-normalizing anything that was already normalized\n",
    "    if str_contains_sub(s, [\"_norm\"]):\n",
    "        continue\n",
    "    # Avoid normalizing one-hot values\n",
    "    if str_contains_sub(s, ['SF10BW125', 'SF11BW125', 'SF12BW125', 'SF7BW125', 'SF8BW125', 'SF8BW500', 'SF9BW125']):\n",
    "        continue\n",
    "    print(f\"Normalizing: {s}...\")\n",
    "    x_train[:, i] = 1 - 2 * (x_train[:, i] - X_TRAIN_MIN[i]) / X_TRAIN_RANGE[i]\n",
    "    x_val[:, i] = 1 - 2 * (x_val[:, i] - X_TRAIN_MIN[i]) / X_TRAIN_RANGE[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples and labels\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "for i in range(3):\n",
    "    sample_idx = np.random.randint(0, y_train.shape[0])\n",
    "    ax = fig.add_subplot(1,3,i+1, projection='3d' if DATA_MODE == \"cartesian\" else None)\n",
    "    ax.scatter(*(y_train[sample_idx, c_idx] for c_idx in range(y_train.shape[-1])), alpha=0.5, label=\"target (label)\")\n",
    "    for hs_idx in range(1,4):\n",
    "        dim_names = [\"lat_norm\", \"long_norm\"] if DATA_MODE == \"geo\" else [\"x_norm\", \"y_norm\", \"z_norm\"]\n",
    "        coord_idxs = [sample_cols.index(f\"hotspot{hs_idx}_{dim_name}\") for dim_name in dim_names]\n",
    "        ax.scatter(*(x_train[sample_idx, c_idx] for c_idx in coord_idxs), alpha=0.5, label=f\"hotspot_{hs_idx}\")\n",
    "        ax.set_xlabel(dim_names[0])\n",
    "        ax.set_ylabel(dim_names[1])\n",
    "        if len(dim_names) > 2:\n",
    "            ax.set_zlabel(dim_names[2])\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Sample: {sample_idx}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "Xtr, Ytr = torch.tensor(x_train).float().to(device), torch.tensor(y_train).float().to(device)\n",
    "Xval, Yval = torch.tensor(x_val).float().to(device), torch.tensor(y_val).float().to(device)\n",
    "Yval_NAIVE = torch.tensor(y_val_naive).float().to(device)\n",
    "def get_batch(batch_size:int =32, is_train: bool=True):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    src_data = Xtr if is_train else Xval\n",
    "    labels = Ytr if is_train else Yval\n",
    "    ix = torch.randint(0, src_data.shape[0], (batch_size,))\n",
    "    return src_data[ix], labels[ix]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Geometric distances between lat/long are NOT linear across the range of lat/long.... so use other coordinates or distance metrics when calculating loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_loss_func(pred_norm_lat_long: Tensor, labels_norm_lat_long: Tensor)->Tensor:\n",
    "    # Convert norm lat/long to cart xyz\n",
    "    pred_xyz = convert_lat_long_to_cart_xyz_TORCH(denorm_lat_long(pred_norm_lat_long))\n",
    "    labels_xyz = convert_lat_long_to_cart_xyz_TORCH(denorm_lat_long(labels_norm_lat_long))\n",
    "    return F.mse_loss(pred_xyz, labels_xyz)\n",
    "\n",
    "def haversine_loss_func(pred_norm_lat_long: Tensor, labels_norm_lat_long: Tensor)->Tensor:\n",
    "    dist = haversine_torch(denorm_lat_long(pred_norm_lat_long), denorm_lat_long(labels_norm_lat_long))\n",
    "    # square to penalize larger errors more significantly\n",
    "    sq_dist = dist ** 2 \n",
    "    return torch.mean(sq_dist)\n",
    "\n",
    "def equirectangular_loss_func(pred_norm_lat_long: Tensor, labels_norm_lat_long: Tensor)->Tensor:\n",
    "    dist = equirectangular_torch(denorm_lat_long(pred_norm_lat_long), denorm_lat_long(labels_norm_lat_long))\n",
    "    # square to penalize larger errors more significantly\n",
    "    sq_dist = dist ** 2 \n",
    "    return torch.mean(sq_dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpTriangulationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        output_size: int, \n",
    "        hidden_layer_size: int = 64, \n",
    "        n_hidden_layers: int = 5, \n",
    "        b_norm: bool = True,\n",
    "        loss_func = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        fan_in_out = [\n",
    "            (input_size, hidden_layer_size)\n",
    "        ] + (n_hidden_layers - 1) * [(hidden_layer_size, hidden_layer_size)]\n",
    "        layers = [\n",
    "            # Flatten the data for each node into a single vector like so: [x1,y1,ss1, x2,y2,ss2...]\n",
    "            nn.Flatten(start_dim=1)\n",
    "        ]\n",
    "        for fan_in, fan_out in fan_in_out:\n",
    "            layers += (\n",
    "                [nn.Linear(fan_in, fan_out, bias=not b_norm)]\n",
    "                + ([nn.BatchNorm1d(fan_out)] if b_norm else [])\n",
    "                + [nn.ReLU()]\n",
    "            )\n",
    "\n",
    "        layers.append(nn.Linear(fan_in_out[-1][-1], output_size))\n",
    "        # parameter init\n",
    "        with torch.no_grad():\n",
    "            layers[-1].weight *= 0.1  # make last layer less confident\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.loss_func = F.mse_loss if loss_func is None else loss_func\n",
    "\n",
    "    def forward(self, samples: Tensor, targets: Tensor = None) -> Tuple[Tensor, Tensor]:\n",
    "        logits = self.layers(samples)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = self.loss_func(logits, targets)\n",
    "            # loss = haversine_loss_func(pred_norm_lat_long=logits, labels_norm_lat_long=targets)\n",
    "            # loss = equirectangular_loss_func(pred_norm_lat_long=logits, labels_norm_lat_long=targets)\n",
    "            # loss = cartesian_loss_func(pred_norm_lat_long=logits, labels_norm_lat_long=targets)\n",
    "            # loss = F.mse_loss(logits, targets)\n",
    "            # loss = F.l1_loss(logits, targets)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MlpTriangulationModel(\n",
    "    input_size = x_train.shape[-1], # number of channels/features in each input sample\n",
    "    output_size = y_train.shape[-1], # number of predicted dimensions... ie: 2 - geo, or 3 - cartesian\n",
    "    loss_func = haversine_loss_func if DATA_MODE == \"geo\" else F.mse_loss\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "MAX_STEPS = 30000\n",
    "LR_DECAY = [MAX_STEPS//15, MAX_STEPS//3, 4*MAX_STEPS//5]\n",
    "PRINT_INTERVAL = MAX_STEPS // 25\n",
    "stats = []\n",
    "\n",
    "def evaluate(data:Tensor, labels:Tensor)->Tuple[float, float]:\n",
    "    with torch.no_grad():\n",
    "        predictions, _ = model(data)\n",
    "    \n",
    "    r_earth = 6373.0 # radius of the earth in km\n",
    "    if DATA_MODE == \"geo\":\n",
    "        dist_err_km = haversine_torch(\n",
    "            denorm_lat_long(predictions), \n",
    "            denorm_lat_long(labels), \n",
    "            radius= 6373.0 # radius of the earth in km\n",
    "        ).cpu().numpy()\n",
    "    elif DATA_MODE == \"cartesian\":\n",
    "        dist_err_norm = np.linalg.norm(predictions.cpu().numpy() - labels.cpu().numpy(), axis=1)\n",
    "        dist_err_km = r_earth * dist_err_norm\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {DATA_MODE}\")\n",
    "    \n",
    "    return dist_err_km.mean(), dist_err_km.std()\n",
    "\n",
    "\n",
    "model.train()\n",
    "for i in range(MAX_STEPS):\n",
    "    # Get a random batch\n",
    "    Xb, Yb = get_batch(batch_size=BATCH_SIZE, is_train=True)\n",
    "\n",
    "    # Predict coordinates and evaluate loss\n",
    "    logits, loss = model(Xb,Yb)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    step_stats = {\n",
    "        \"step\": i,\n",
    "        \"loss\": loss.item()\n",
    "    }\n",
    "    if i % PRINT_INTERVAL == 0:\n",
    "        with torch.no_grad():\n",
    "            val_error_mean, val_error_std = evaluate(data=Xval, labels=Yval)\n",
    "            step_stats[\"val_error_dist_km_mean\"] = val_error_mean\n",
    "            step_stats[\"val_error_dist_km_std\"] = val_error_std\n",
    "        print(f\"Step: {i},\\t Loss: {loss.item():.6f},\\tValErrDist: mean = {val_error_mean:.4f} km (std:: {val_error_std:.4f})\")\n",
    "    if i in LR_DECAY:\n",
    "        print(\"Adjusting LR\")\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] *= 0.1\n",
    "    stats.append(step_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contents = {}\n",
    "for i, step_stats in enumerate(stats):\n",
    "    step = step_stats.pop(\"step\", i)\n",
    "    for k, v in step_stats.items():\n",
    "        if k not in plot_contents:\n",
    "            plot_contents[k] = {\"x\": [], \"y\":[]}\n",
    "        plot_contents[k][\"x\"].append(step)\n",
    "        plot_contents[k][\"y\"].append(v)\n",
    "for k, v in plot_contents.items():\n",
    "    x, y = np.asarray(v[\"x\"]), np.asarray(v[\"y\"])\n",
    "    plt.figure()\n",
    "    plt.title(k)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"global step\")\n",
    "    plt.ylabel(k)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "samples, labels = get_batch(batch_size=3, is_train=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions, _ = model(samples)\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "dim_names = [\"lat_norm\", \"long_norm\"] if DATA_MODE == \"geo\" else [\"x_norm\", \"y_norm\", \"z_norm\"]\n",
    "for i, sample, label, pred in zip(list(range(len(samples))), samples.cpu(), labels.cpu(), predictions.cpu()):\n",
    "    ax = fig.add_subplot(1,3,i+1, projection='3d' if DATA_MODE == \"cartesian\" else None)\n",
    "\n",
    "    hs_coords = [\n",
    "        [sample[sample_cols.index(f\"hotspot{i}_{suffix}\")] for i in range(1,4)] for suffix in dim_names\n",
    "    ]\n",
    "    r_earth_km = 6373.0 # radius of the earth in km\n",
    "    if DATA_MODE == \"geo\":\n",
    "        dist_err_km = haversine_np(\n",
    "            np.expand_dims(pred.numpy(),0), \n",
    "            np.expand_dims(label.numpy(),0), \n",
    "            radius=r_earth_km\n",
    "        )[0]\n",
    "    elif DATA_MODE == \"cartesian\":\n",
    "        dist_err_km = r_earth_km * np.linalg.norm(np.expand_dims(pred.numpy(),0) - np.expand_dims(label.numpy(),0), axis=1)[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {DATA_MODE}\")\n",
    "    \n",
    "    ax = fig.add_subplot(1,3,i+1, projection='3d' if DATA_MODE == \"cartesian\" else None)\n",
    "    ax.set_title(f\"Prediction - Err: {dist_err_km:.1f}km\")\n",
    "    ax.set_xlabel(dim_names[0])\n",
    "    ax.set_ylabel(dim_names[1])\n",
    "    if len(dim_names) > 2:\n",
    "        ax.set_zlabel(dim_names[2])\n",
    "    ax.scatter(\n",
    "        *tuple(hs_coords),\n",
    "        alpha=0.5,\n",
    "        label=\"hotspot\",\n",
    "    )\n",
    "    ax.scatter(*tuple(label), alpha=0.5, label=\"target/label\")\n",
    "    ax.scatter(*tuple(pred), alpha=0.5, label=\"pred - model\")\n",
    "    mid = (label + pred) / 2\n",
    "    if len(mid) == 2:\n",
    "        ax.annotate(f\"err={dist_err_km:.1f}km\", xy=tuple(mid))\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors for the entire validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions, _ = model(Xval)\n",
    "r_earth = 6373.0 # radius of the earth in km\n",
    "dist_err_km = haversine_torch(\n",
    "    denorm_lat_long(predictions), \n",
    "    denorm_lat_long(Yval), \n",
    "    radius= 6373.0 # radius of the earth in km\n",
    ").cpu().numpy()\n",
    "\n",
    "naive_tri_dist_err_km = haversine_torch(\n",
    "    denorm_lat_long(Yval_NAIVE), \n",
    "    denorm_lat_long(Yval), \n",
    "    radius= 6373.0 # radius of the earth in km\n",
    ").cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution:\n",
    "plt.hist(dist_err_km, bins=1000, label=\"model\")\n",
    "plt.hist(naive_tri_dist_err_km, bins=1000, label=\"naive trilat\")\n",
    "plt.title(\"Prediction Error - Great Circle Distance\")\n",
    "plt.xlabel(\"Error Distance (km)\")\n",
    "plt.ylabel(\"# of Preds\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in\n",
    "plt.figure()\n",
    "plt.hist(dist_err_km, bins=1000, label=\"model\")\n",
    "plt.hist(naive_tri_dist_err_km, bins=1000, label=\"naive trilat\")\n",
    "plt.title(\"Prediction Error - Great Circle Distance - q:0.99\")\n",
    "plt.xlim(0, np.quantile(dist_err_km, 0.99))\n",
    "plt.xlabel(\"Error Distance (km)\")\n",
    "plt.ylabel(\"# of Preds\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Prediction Error (Density) - Great Circle Distance\")\n",
    "plt.hist(dist_err_km, bins=100, alpha=0.5, density=True, cumulative=True, label=\"model\")\n",
    "plt.hist(naive_tri_dist_err_km, bins=100, alpha=0.5, density=True, cumulative=True, label=\"naive trilat\")\n",
    "plt.xlabel(\"Threshold - Error Distance (km)\")\n",
    "plt.ylabel(\"Portion of preds below threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2b0c702ac25f97acd8a4a661cbe334755fdd1897c380bb354f2173ef576db5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
