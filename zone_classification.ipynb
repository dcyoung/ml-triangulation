{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from common import haversine_cluster, str_contains_sub, haversine_np, RADIUS_EARTH_KM, normalize_lat_long\n",
    "from models import MlpTriangulationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/mappers_processed.csv\")\n",
    "if \"id\" in df.columns:\n",
    "    df = df.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_clusters = \"data/clusters_sfbay_20230211-170608.csv\"\n",
    "df_clusters = pd.read_csv(fpath_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit data points to those that fall within the clusterable regions\n",
    "lat_bounds = [df_clusters[\"cluster_lat\"].min(), df_clusters[\"cluster_lat\"].max()]\n",
    "long_bounds = [df_clusters[\"cluster_long\"].min(), df_clusters[\"cluster_long\"].max()]\n",
    "\n",
    "for prefix in {\"target\", \"hotspot1\", \"hotspot2\", \"hotspot3\"}:\n",
    "    df = df[df[f\"{prefix}_lat\"] > min(lat_bounds)]\n",
    "    df = df[df[f\"{prefix}_lat\"] < max(lat_bounds)]\n",
    "    df = df[df[f\"{prefix}_long\"] > min(long_bounds)]\n",
    "    df = df[df[f\"{prefix}_long\"] < max(long_bounds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.sample(n=min(len(df), 5000))\n",
    "for prefix in [\"target\"] + [f\"hotspot{i}\" for i in range(1,4)]:\n",
    "    plt.scatter(subset[f\"{prefix}_long\"], subset[f\"{prefix}_lat\"], s=2, alpha=0.4, label=f\"{prefix}\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Random sampling of data points\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"naive_tri_alt\" in df.columns:\n",
    "    df = df.rename(columns={\"naive_tri_alt\": \"naive_tri_altitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster assignments and their normalized values\n",
    "for prefix in [\"target\", \"hotspot1\", \"hotspot2\", \"hotspot3\", \"centroid_projected\", \"naive_tri\"]:\n",
    "    # Assign a cluster_id\n",
    "    df[f\"{prefix}_cluster_id\"] = haversine_cluster(\n",
    "        points_lat_long_deg=df[[f\"{prefix}_lat\", f\"{prefix}_long\"]].to_numpy(),\n",
    "        centroids_lat_long_deg=df_clusters[[\"cluster_lat\", \"cluster_long\"]].to_numpy(),\n",
    "        trace=True,\n",
    "    )\n",
    "    # normalize cluster_id\n",
    "    df[f\"{prefix}_cluster_id_norm\"] = 2 * df[f\"{prefix}_cluster_id\"] / (len(df_clusters) - 1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalized lat, long\n",
    "for prefix in [\"target\", \"hotspot1\", \"hotspot2\", \"hotspot3\", \"centroid_projected\", \"naive_tri\"]:\n",
    "    df[[f\"{prefix}_lat_norm\", f\"{prefix}_long_norm\"]] = normalize_lat_long(\n",
    "        lat_long_deg=df[[f\"{prefix}_lat\", f\"{prefix}_long\"]].to_numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test with 80 train / 20 test\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"target_cluster_id\"]\n",
    "naive_tri_cols = [\"naive_tri_cluster_id\"]\n",
    "sample_cols = [\n",
    "        'naive_tri_cluster_id_norm', 'centroid_projected_cluster_id_norm', 'hotspot1_cluster_id_norm', 'hotspot2_cluster_id_norm', 'hotspot3_cluster_id_norm', \n",
    "        'centroid_projected_lat_norm', 'centroid_projected_long_norm',\n",
    "        'hotspot1_lat_norm', 'hotspot1_long_norm', \n",
    "        'hotspot2_lat_norm', 'hotspot2_long_norm', \n",
    "        'hotspot3_lat_norm', 'hotspot3_long_norm', \n",
    "        'hotspot1_rssi', 'hotspot2_rssi', 'hotspot3_rssi', \n",
    "        'hotspot1_snr', 'hotspot2_snr','hotspot3_snr',\n",
    "        # 'hotspot1_fspl_dist_km', 'hotspot2_fspl_dist_km', 'hotspot3_fspl_dist_km',\n",
    "        'naive_tri_lat_norm', 'naive_tri_long_norm',\n",
    "]\n",
    "\n",
    "y_train = df_train[label_cols].to_numpy()\n",
    "y_val_cluster = df_val[label_cols].to_numpy()\n",
    "y_val_ll = df_val[[\"target_lat\", \"target_long\"]].to_numpy()\n",
    "y_val_naive_cluster = df_val[naive_tri_cols].to_numpy()\n",
    "y_val_naive_ll = df_val[[\"naive_tri_lat\", \"naive_tri_long\"]].to_numpy()\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train.min(axis=0))\n",
    "print(y_train.max(axis=0))\n",
    "print(y_val_cluster.shape)\n",
    "print(y_val_cluster.min(axis=0))\n",
    "print(y_val_cluster.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = df_train[sample_cols].to_numpy()\n",
    "x_val = df_val[sample_cols].to_numpy()\n",
    "\n",
    "# Normalize all features to expected range -1:1 based on consideration of the training data alone\n",
    "X_TRAIN_MIN = x_train.min(axis=0)\n",
    "X_TRAIN_RANGE = x_train.ptp(axis=0)\n",
    "\n",
    "for i, s in enumerate(sample_cols):\n",
    "    # Avoid re-normalizing anything that was already normalized\n",
    "    if str_contains_sub(s, [\"_norm\"]):\n",
    "        continue\n",
    "    print(f\"Normalizing: {s}...\")\n",
    "    x_train[:, i] = 2 * (x_train[:, i] - X_TRAIN_MIN[i]) / X_TRAIN_RANGE[i] - 1\n",
    "    x_val[:, i] = 2 * (x_val[:, i] - X_TRAIN_MIN[i]) / X_TRAIN_RANGE[i] - 1\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train.min(axis=0))\n",
    "print(x_train.max(axis=0))\n",
    "print(x_val.shape)\n",
    "print(x_val.min(axis=0))\n",
    "print(x_val.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "Xtr, Ytr = torch.tensor(x_train).float().to(device), torch.tensor(y_train).to(device)\n",
    "Xval, Yval = torch.tensor(x_val).float().to(device), torch.tensor(y_val_cluster).to(device)\n",
    "Yval_NAIVE = torch.tensor(y_val_naive_cluster).float().to(device)\n",
    "\n",
    "def get_batch(batch_size:int =32, is_train: bool=True):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    src_data = Xtr if is_train else Xval\n",
    "    labels = Ytr if is_train else Yval\n",
    "    ix = torch.randint(0, src_data.shape[0], (batch_size,))\n",
    "    return src_data[ix], labels[ix].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MlpTriangulationModel(\n",
    "    input_size = x_train.shape[-1], # number of channels/features in each input sample\n",
    "    output_size = len(df_clusters), # number of predictable clusters\n",
    ")\n",
    "model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# Create optimizer\n",
    "init_lr = 3e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a good learning rate\n",
    "if False:\n",
    "    lri = []\n",
    "    lossi = []\n",
    "    lre = torch.linspace(-6, -2, 1000)\n",
    "    lrs = 10**lre\n",
    "    BATCH_SIZE = 256\n",
    "    for i, lr in enumerate(lrs):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "        # Get a random batch\n",
    "        Xb, Yb = get_batch(batch_size=BATCH_SIZE, is_train=True)\n",
    "\n",
    "        # Predict coordinates and evaluate loss\n",
    "        logits, loss = model(Xb,Yb.squeeze())\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lri.append(lr.item())\n",
    "        lossi.append(loss.item())\n",
    "\n",
    "    plt.plot(lri, lossi)\n",
    "    plt.xlabel(\"Learning Rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_val_acc = (Yval.squeeze() == Yval_NAIVE.squeeze()).sum().item()/Yval.size(0)\n",
    "print(f\"NAIVE Trilat Val Cluster Cls Acc: {100*naive_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "MAX_STEPS = 20000\n",
    "LR_DECAY = [6000, 12000]\n",
    "PRINT_INTERVAL = MAX_STEPS // 25\n",
    "stats = []\n",
    "\n",
    "def eval_accuracy(data:Tensor, labels:Tensor)->Tuple[float, float]:\n",
    "    with torch.no_grad():\n",
    "        predictions, _ = model(data)\n",
    "    # get highest scores & their cluster idx\n",
    "    # cluster_predictions = torch.argmax(predictions, dim=1)\n",
    "    scores, predicted_cluster_idxs = predictions.max(dim=1)\n",
    "    n = data.size(0)\n",
    "    assert predicted_cluster_idxs.size(0) == n\n",
    "    return (predicted_cluster_idxs == labels).sum().item()/n\n",
    "\n",
    "model.train()\n",
    "lr = init_lr\n",
    "for i in range(MAX_STEPS):\n",
    "    # Get a random batch\n",
    "    Xb, Yb = get_batch(batch_size=BATCH_SIZE, is_train=True)\n",
    "\n",
    "    # Predict coordinates and evaluate loss\n",
    "    logits, loss = model(Xb,Yb)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    step_stats = {\n",
    "        \"step\": i,\n",
    "        \"loss\": loss.item(),\n",
    "        \"lr\": lr\n",
    "    }\n",
    "    if i % PRINT_INTERVAL == 0:\n",
    "        with torch.no_grad():\n",
    "            classification_accuracy = eval_accuracy(data=Xval, labels=Yval.squeeze())\n",
    "            step_stats[\"val_cluster_classification_accuracy\"] = classification_accuracy\n",
    "        print(f\"Step: {i},\\t Loss: {loss.item():.6f},\\tVal Cluster Cls Acc: {100*classification_accuracy:.2f}%\")\n",
    "    if i in LR_DECAY:\n",
    "        print(\"Adjusting LR\")\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = lr * 0.1\n",
    "            g['lr'] *= 0.1\n",
    "    stats.append(step_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contents = {}\n",
    "for i, step_stats in enumerate(stats):\n",
    "    step = step_stats.pop(\"step\", i)\n",
    "    for k, v in step_stats.items():\n",
    "        if k not in plot_contents:\n",
    "            plot_contents[k] = {\"x\": [], \"y\":[]}\n",
    "        plot_contents[k][\"x\"].append(step)\n",
    "        plot_contents[k][\"y\"].append(v)\n",
    "for k, v in plot_contents.items():\n",
    "    x, y = np.asarray(v[\"x\"]), np.asarray(v[\"y\"])\n",
    "    if k == \"loss\":\n",
    "        x = x[1000:]\n",
    "        y = y[1000:]\n",
    "    plt.figure()\n",
    "    plt.title(k)\n",
    "    plt.plot(x,y)\n",
    "    n = y.shape[0] // 100\n",
    "    if n > 10:\n",
    "        smoothed = np.convolve(y, np.ones(n), 'valid') / n\n",
    "        smoothed = np.average(np.lib.stride_tricks.sliding_window_view(y, n), axis=1)\n",
    "        plt.plot(x[n-1:], smoothed, label=\"Smoothed\")\n",
    "    plt.xlabel(\"global step\")\n",
    "    plt.ylabel(k)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything needed for inference\n",
    "now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_dir = Path(\"models\") / now_str\n",
    "output_dir.mkdir(exist_ok=False)\n",
    "\n",
    "# Save the clusters csv\n",
    "copyfile(fpath_clusters, output_dir/\"clusters.csv\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model, output_dir/\"model.pth\")\n",
    "\n",
    "# Save training stats\n",
    "with open(output_dir/\"training_stats.json\", \"w\") as f:\n",
    "    json.dump(stats, f)\n",
    "\n",
    "# Save metadata\n",
    "with open(output_dir/\"metadata.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "       \"zone\": {\n",
    "            \"latitude\": {\n",
    "                \"min\": min(lat_bounds),\n",
    "                \"max\": max(lat_bounds)\n",
    "            },\n",
    "            \"longitude\": {\n",
    "                \"min\": min(long_bounds),\n",
    "                \"max\": max(long_bounds)\n",
    "            }\n",
    "       },\n",
    "       \"model\": {\n",
    "            \"output_features\": label_cols,\n",
    "            \"input_features\": sample_cols, \n",
    "            \"input_normalization\": {\n",
    "                \"input_features_min\": list(X_TRAIN_MIN),\n",
    "                \"input_features_range\": list(X_TRAIN_RANGE),\n",
    "            }\n",
    "       },\n",
    "    }, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model from disk\n",
    "model = torch.load(output_dir/\"model.pth\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions, _ = model(Xval)\n",
    "predicted_cluster_idxs = predictions.max(dim=1)[-1].squeeze().cpu().numpy()\n",
    "predicted_lat_long = df_clusters[[\"cluster_lat\", \"cluster_long\"]].to_numpy()[predicted_cluster_idxs, :]\n",
    "\n",
    "dist_err_km = haversine_np(\n",
    "    predicted_lat_long, \n",
    "    y_val_ll, \n",
    "    radius=RADIUS_EARTH_KM\n",
    ")\n",
    "naive_tri_dist_err_km = haversine_np(\n",
    "    y_val_naive_ll, \n",
    "    y_val_ll, \n",
    "    radius=RADIUS_EARTH_KM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution:\n",
    "n_bins = 500\n",
    "plt.hist(dist_err_km, bins=n_bins, alpha=0.7, label=\"model\")\n",
    "plt.hist(naive_tri_dist_err_km, bins=n_bins, alpha=0.7, label=\"naive trilat\")\n",
    "plt.title(\"Prediction Error - Haversine Distance\")\n",
    "plt.xlabel(\"Error Distance (km)\")\n",
    "plt.ylabel(\"# of Preds\")\n",
    "# plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in\n",
    "plt.figure()\n",
    "plt.hist(dist_err_km, bins=n_bins, alpha=0.7, label=\"model\")\n",
    "plt.hist(naive_tri_dist_err_km, bins=n_bins, alpha=0.7, label=\"naive trilat\")\n",
    "plt.title(\"Prediction Error - Haversine Distance - q:0.99\")\n",
    "plt.xlim(0, np.quantile(dist_err_km, 0.99))\n",
    "plt.xlabel(\"Error Distance (km)\")\n",
    "plt.ylabel(\"# of Preds\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Prediction Error (Density) - Haversine Distance\")\n",
    "plt.hist(dist_err_km, bins=n_bins, alpha=0.5, density=True, cumulative=True, label=\"model\")\n",
    "plt.hist(naive_tri_dist_err_km, bins=n_bins, alpha=0.5, density=True, cumulative=True, label=\"naive trilat\")\n",
    "plt.xlabel(\"Threshold - Error Haversine Distance (km)\")\n",
    "plt.ylabel(\"Portion of preds below threshold\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2b0c702ac25f97acd8a4a661cbe334755fdd1897c380bb354f2173ef576db5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
